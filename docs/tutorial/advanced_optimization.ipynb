{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Optimization Techniques\n",
    "\n",
    "This tutorial covers advanced optimization techniques for writing high-performance brain dynamics operators with `braintaichi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:05.334967Z",
     "start_time": "2025-10-29T09:31:04.159011Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import taichi as ti\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import braintaichi as bti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loop Configuration and Parallelization\n",
    "\n",
    "Taichi automatically parallelizes outer loops, but you can fine-tune the parallelization behavior for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Serial vs Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:07.509127Z",
     "start_time": "2025-10-29T09:31:07.505121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Serial execution (useful when order matters or using break statements)\n",
    "@ti.kernel\n",
    "def serial_sum(\n",
    "    arr: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    total = 0.0\n",
    "    ti.loop_config(serialize=True)\n",
    "    for i in range(arr.shape[0]):\n",
    "        total += arr[i]\n",
    "        if total > 100.0:  # Can use break in serial loops\n",
    "            break\n",
    "    out[0] = total\n",
    "\n",
    "# Parallel execution (default for outer loops)\n",
    "@ti.kernel\n",
    "def parallel_multiply(\n",
    "    arr: ti.types.ndarray(ndim=1),\n",
    "    scalar: ti.f32,\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    # This loop is automatically parallelized\n",
    "    for i in range(arr.shape[0]):\n",
    "        out[i] = arr[i] * scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Block Dimension Tuning for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:10.601424Z",
     "start_time": "2025-10-29T09:31:10.597920Z"
    }
   },
   "outputs": [],
   "source": [
    "@ti.kernel\n",
    "def optimized_gpu_kernel(\n",
    "    data: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    # Configure block dimension for GPU\n",
    "    # Common values: 64, 128, 256, 512\n",
    "    ti.loop_config(block_dim=256)\n",
    "    for i in range(data.shape[0]):\n",
    "        out[i] = ti.sqrt(data[i]) + ti.sin(data[i])\n",
    "\n",
    "# For CPU, configure number of parallel threads\n",
    "@ti.kernel\n",
    "def optimized_cpu_kernel(\n",
    "    data: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    ti.loop_config(parallelize=8)  # Use 8 threads\n",
    "    for i in range(data.shape[0]):\n",
    "        out[i] = ti.sqrt(data[i]) + ti.sin(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Access Optimization\n",
    "\n",
    "Efficient memory access patterns are crucial for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Coalesced Memory Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:20.514630Z",
     "start_time": "2025-10-29T09:31:20.510653Z"
    }
   },
   "outputs": [],
   "source": [
    "# BAD: Non-coalesced access (strided access)\n",
    "@ti.kernel\n",
    "def bad_memory_access(\n",
    "    matrix: ti.types.ndarray(ndim=2),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    n, m = matrix.shape\n",
    "    for col in range(m):\n",
    "        total = 0.0\n",
    "        for row in range(n):\n",
    "            total += matrix[row, col]  # Column-wise access (non-coalesced)\n",
    "        out[col] = total\n",
    "\n",
    "# GOOD: Coalesced access (contiguous access)\n",
    "@ti.kernel\n",
    "def good_memory_access(\n",
    "    matrix: ti.types.ndarray(ndim=2),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    n, m = matrix.shape\n",
    "    for row in range(n):\n",
    "        for col in range(m):\n",
    "            out[col] += matrix[row, col]  # Row-wise access (coalesced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using Local Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:22.972325Z",
     "start_time": "2025-10-29T09:31:22.968465Z"
    }
   },
   "outputs": [],
   "source": [
    "# BAD: Multiple global memory accesses\n",
    "@ti.kernel\n",
    "def bad_global_access(\n",
    "    arr: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(arr.shape[0]):\n",
    "        out[i] = arr[i] * arr[i] + arr[i] * 2.0  # arr[i] accessed 3 times\n",
    "\n",
    "# GOOD: Use local variable to cache value\n",
    "@ti.kernel\n",
    "def good_local_cache(\n",
    "    arr: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(arr.shape[0]):\n",
    "        val = arr[i]  # Load once into local variable\n",
    "        out[i] = val * val + val * 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizing Sparse Operations\n",
    "\n",
    "Sparse operations are common in brain dynamics. Here are optimization strategies for sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimized CSR Matrix-Vector Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:26.181144Z",
     "start_time": "2025-10-29T09:31:26.175562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard implementation\n",
    "@ti.kernel\n",
    "def csr_matvec_standard(\n",
    "    values: ti.types.ndarray(ndim=1),\n",
    "    indices: ti.types.ndarray(ndim=1),\n",
    "    indptr: ti.types.ndarray(ndim=1),\n",
    "    vector: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for row in range(indptr.shape[0] - 1):\n",
    "        row_sum = 0.0\n",
    "        for j in range(indptr[row], indptr[row + 1]):\n",
    "            row_sum += values[j] * vector[indices[j]]\n",
    "        out[row] = row_sum\n",
    "\n",
    "# Optimized implementation with local caching\n",
    "@ti.kernel\n",
    "def csr_matvec_optimized(\n",
    "    values: ti.types.ndarray(ndim=1),\n",
    "    indices: ti.types.ndarray(ndim=1),\n",
    "    indptr: ti.types.ndarray(ndim=1),\n",
    "    vector: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for row in range(indptr.shape[0] - 1):\n",
    "        row_sum = 0.0\n",
    "        start = indptr[row]\n",
    "        end = indptr[row + 1]\n",
    "        for j in range(start, end):\n",
    "            col = indices[j]\n",
    "            val = values[j]\n",
    "            vec_val = vector[col]\n",
    "            row_sum += val * vec_val\n",
    "        out[row] = row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:32.073625Z",
     "start_time": "2025-10-29T09:31:29.664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard implementation: 0.200 ms\n",
      "Optimized implementation: 0.300 ms\n",
      "Speedup: 0.67x\n"
     ]
    }
   ],
   "source": [
    "# Benchmark the two implementations\n",
    "def benchmark_csr_matvec():\n",
    "    n = 10000\n",
    "    density = 0.01\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    dense_mat = (np.random.rand(n, n) < density).astype(float) * np.random.rand(n, n)\n",
    "    sparse_mat = csr_matrix(dense_mat)\n",
    "    vector = np.random.rand(n).astype(np.float32)\n",
    "    \n",
    "    # Register operators\n",
    "    op_standard = bti.XLACustomOp(cpu_kernel=csr_matvec_standard, gpu_kernel=csr_matvec_standard)\n",
    "    op_optimized = bti.XLACustomOp(cpu_kernel=csr_matvec_optimized, gpu_kernel=csr_matvec_optimized)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    values = jnp.array(sparse_mat.data, dtype=jnp.float32)\n",
    "    indices = jnp.array(sparse_mat.indices, dtype=jnp.int32)\n",
    "    indptr = jnp.array(sparse_mat.indptr, dtype=jnp.int32)\n",
    "    vec = jnp.array(vector, dtype=jnp.float32)\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(3):\n",
    "        _ = op_standard(values, indices, indptr, vec, \n",
    "                       outs=[jax.ShapeDtypeStruct((n,), dtype=jnp.float32)])\n",
    "        _ = op_optimized(values, indices, indptr, vec,\n",
    "                        outs=[jax.ShapeDtypeStruct((n,), dtype=jnp.float32)])\n",
    "    \n",
    "    # Benchmark\n",
    "    n_runs = 10\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = op_standard(values, indices, indptr, vec,\n",
    "                       outs=[jax.ShapeDtypeStruct((n,), dtype=jnp.float32)])\n",
    "    time_standard = (time.time() - start) / n_runs\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = op_optimized(values, indices, indptr, vec,\n",
    "                        outs=[jax.ShapeDtypeStruct((n,), dtype=jnp.float32)])\n",
    "    time_optimized = (time.time() - start) / n_runs\n",
    "    \n",
    "    print(f\"Standard implementation: {time_standard*1000:.3f} ms\")\n",
    "    print(f\"Optimized implementation: {time_optimized*1000:.3f} ms\")\n",
    "    print(f\"Speedup: {time_standard/time_optimized:.2f}x\")\n",
    "\n",
    "benchmark_csr_matvec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Event-Driven Optimization\n",
    "\n",
    "Event-driven computations are essential for spiking neural networks. Here's how to optimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:34.149288Z",
     "start_time": "2025-10-29T09:31:34.145290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Method 1: Direct event checking (simple but may have branch divergence)\n",
    "@ti.kernel\n",
    "def event_driven_v1(\n",
    "    indices: ti.types.ndarray(ndim=1),\n",
    "    indptr: ti.types.ndarray(ndim=1),\n",
    "    events: ti.types.ndarray(ndim=1),\n",
    "    weight: ti.f32,\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    ti.loop_config(serialize=True)\n",
    "    for row in range(indptr.shape[0] - 1):\n",
    "        if events[row]:  # Check event\n",
    "            for j in range(indptr[row], indptr[row + 1]):\n",
    "                out[indices[j]] += weight\n",
    "\n",
    "# Method 2: Event filtering (better for low firing rates)\n",
    "@ti.kernel\n",
    "def event_driven_v2(\n",
    "    indices: ti.types.ndarray(ndim=1),\n",
    "    indptr: ti.types.ndarray(ndim=1),\n",
    "    event_indices: ti.types.ndarray(ndim=1),  # Indices of neurons that fired\n",
    "    weight: ti.f32,\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    # Only iterate over neurons that actually fired\n",
    "    ti.loop_config(serialize=True)\n",
    "    for i in range(event_indices.shape[0]):\n",
    "        row = event_indices[i]\n",
    "        for j in range(indptr[row], indptr[row + 1]):\n",
    "            out[indices[j]] += weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:37.550565Z",
     "start_time": "2025-10-29T09:31:36.576408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons: 10000\n",
      "Connectivity density: 0.01\n",
      "Firing rate: 0.05\n",
      "Neurons that fired: 511\n",
      "\n",
      "Method 1: Direct event checking\n",
      "Method 2: Event filtering (recommended for low firing rates)\n"
     ]
    }
   ],
   "source": [
    "# Benchmark event-driven methods\n",
    "def benchmark_event_driven():\n",
    "    n = 10000\n",
    "    density = 0.01\n",
    "    firing_rate = 0.05  # 5% of neurons fire\n",
    "    \n",
    "    # Create connectivity\n",
    "    conn = (np.random.rand(n, n) < density).astype(float)\n",
    "    sparse_conn = csr_matrix(conn)\n",
    "    \n",
    "    # Create events\n",
    "    events = np.random.rand(n) < firing_rate\n",
    "    event_indices = np.where(events)[0].astype(np.int32)\n",
    "    \n",
    "    # Register operators\n",
    "    op_v1 = bti.XLACustomOp(cpu_kernel=event_driven_v1, gpu_kernel=event_driven_v1)\n",
    "    op_v2 = bti.XLACustomOp(cpu_kernel=event_driven_v2, gpu_kernel=event_driven_v2)\n",
    "    \n",
    "    print(f\"Number of neurons: {n}\")\n",
    "    print(f\"Connectivity density: {density}\")\n",
    "    print(f\"Firing rate: {firing_rate}\")\n",
    "    print(f\"Neurons that fired: {len(event_indices)}\")\n",
    "    print(\"\\nMethod 1: Direct event checking\")\n",
    "    print(\"Method 2: Event filtering (recommended for low firing rates)\")\n",
    "\n",
    "benchmark_event_driven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Type Optimization\n",
    "\n",
    "Choosing the right data types can significantly impact performance and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:39.733645Z",
     "start_time": "2025-10-29T09:31:39.729645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using different precision levels\n",
    "@ti.kernel\n",
    "def compute_f32(\n",
    "    arr: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(arr.shape[0]):\n",
    "        val = ti.cast(arr[i], ti.f32)  # Explicitly cast to float32\n",
    "        out[i] = ti.sqrt(val) + ti.sin(val)\n",
    "\n",
    "@ti.kernel\n",
    "def compute_f64(\n",
    "    arr: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(arr.shape[0]):\n",
    "        val = ti.cast(arr[i], ti.f64)  # Explicitly cast to float64\n",
    "        out[i] = ti.sqrt(val) + ti.sin(val)\n",
    "\n",
    "# Using integer types for indices\n",
    "@ti.kernel\n",
    "def gather_operation(\n",
    "    data: ti.types.ndarray(ndim=1),\n",
    "    indices: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(indices.shape[0]):\n",
    "        idx = ti.cast(indices[i], ti.i32)  # Use int32 for indices\n",
    "        out[i] = data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Atomic Operations for Race Condition Handling\n",
    "\n",
    "When multiple threads need to update the same memory location, use atomic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:42.755838Z",
     "start_time": "2025-10-29T09:31:42.752606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Without atomic operations (may have race conditions)\n",
    "@ti.kernel\n",
    "def scatter_add_unsafe(\n",
    "    values: ti.types.ndarray(ndim=1),\n",
    "    indices: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(values.shape[0]):\n",
    "        idx = indices[i]\n",
    "        out[idx] += values[i]  # Race condition if multiple threads write to same idx\n",
    "\n",
    "# With atomic operations (safe)\n",
    "@ti.kernel\n",
    "def scatter_add_safe(\n",
    "    values: ti.types.ndarray(ndim=1),\n",
    "    indices: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(values.shape[0]):\n",
    "        idx = indices[i]\n",
    "        ti.atomic_add(out[idx], values[i])  # Thread-safe atomic addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Helper Functions for Code Reusability\n",
    "\n",
    "Use `@ti.func` to create reusable helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:46.300396Z",
     "start_time": "2025-10-29T09:31:46.295889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "@ti.func\n",
    "def relu(x: ti.f32) -> ti.f32:\n",
    "    return ti.max(0.0, x)\n",
    "\n",
    "@ti.func\n",
    "def sigmoid(x: ti.f32) -> ti.f32:\n",
    "    return 1.0 / (1.0 + ti.exp(-x))\n",
    "\n",
    "@ti.func\n",
    "def leaky_relu(x: ti.f32, alpha: ti.f32) -> ti.f32:\n",
    "    return ti.max(alpha * x, x)\n",
    "\n",
    "# Use helper functions in kernels\n",
    "@ti.kernel\n",
    "def apply_activation(\n",
    "    data: ti.types.ndarray(ndim=1),\n",
    "    activation_type: ti.i32,\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    for i in range(data.shape[0]):\n",
    "        x = data[i]\n",
    "        if activation_type == 0:\n",
    "            out[i] = relu(x)\n",
    "        elif activation_type == 1:\n",
    "            out[i] = sigmoid(x)\n",
    "        else:\n",
    "            out[i] = leaky_relu(x, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices Summary\n",
    "\n",
    "### Performance Checklist:\n",
    "\n",
    "1. **Loop Configuration**\n",
    "   - Use parallel loops for independent operations\n",
    "   - Use serial loops when order matters or for break statements\n",
    "   - Tune `block_dim` for GPU (try 128, 256, 512)\n",
    "\n",
    "2. **Memory Access**\n",
    "   - Prefer coalesced (contiguous) memory access\n",
    "   - Cache frequently accessed values in local variables\n",
    "   - Minimize global memory accesses\n",
    "\n",
    "3. **Data Types**\n",
    "   - Use `ti.f32` instead of `ti.f64` when precision allows\n",
    "   - Use `ti.i32` for indices\n",
    "   - Explicit casting for clarity\n",
    "\n",
    "4. **Sparse Operations**\n",
    "   - Store data in efficient formats (CSR, COO)\n",
    "   - Use event filtering for low firing rates\n",
    "   - Cache row/column indices when possible\n",
    "\n",
    "5. **Synchronization**\n",
    "   - Use atomic operations for concurrent writes\n",
    "   - Avoid unnecessary synchronization\n",
    "\n",
    "6. **Code Organization**\n",
    "   - Extract common operations into `@ti.func` helpers\n",
    "   - Keep kernels focused and modular\n",
    "   - Profile before optimizing\n",
    "\n",
    "### Profiling Tips:\n",
    "\n",
    "```python\n",
    "# Enable profiling\n",
    "import time\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    result = your_operator(...)\n",
    "\n",
    "# Measure\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = your_operator(...)\n",
    "elapsed = (time.time() - start) / 100\n",
    "print(f\"Average time: {elapsed*1000:.3f} ms\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Example: Optimized Spiking Neural Network Layer\n",
    "\n",
    "Let's combine all the optimization techniques into a complete, optimized SNN layer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:51.124761Z",
     "start_time": "2025-10-29T09:31:51.118997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for neuron dynamics\n",
    "@ti.func\n",
    "def lif_dynamics(v: ti.f32, current: ti.f32, tau: ti.f32, dt: ti.f32) -> ti.f32:\n",
    "    \"\"\"Leaky Integrate-and-Fire neuron dynamics\"\"\"\n",
    "    return v + ((-v + current) / tau) * dt\n",
    "\n",
    "@ti.func\n",
    "def check_spike(v: ti.f32, threshold: ti.f32) -> ti.i32:\n",
    "    \"\"\"Check if neuron spikes\"\"\"\n",
    "    return 1 if v >= threshold else 0\n",
    "\n",
    "# Optimized SNN layer kernel\n",
    "@ti.kernel\n",
    "def snn_layer_optimized(\n",
    "    # Synaptic connectivity (CSR format)\n",
    "    syn_values: ti.types.ndarray(ndim=1),\n",
    "    syn_indices: ti.types.ndarray(ndim=1),\n",
    "    syn_indptr: ti.types.ndarray(ndim=1),\n",
    "    # Input spikes from previous layer\n",
    "    input_spikes: ti.types.ndarray(ndim=1),\n",
    "    # Neuron states\n",
    "    membrane_v: ti.types.ndarray(ndim=1),\n",
    "    # Parameters (as 0-dim arrays)\n",
    "    tau: ti.types.ndarray(),\n",
    "    threshold: ti.types.ndarray(),\n",
    "    dt: ti.types.ndarray(),\n",
    "    # Outputs\n",
    "    output_spikes: ti.types.ndarray(ndim=1),\n",
    "    new_membrane_v: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    n_neurons = membrane_v.shape[0]\n",
    "    tau_val = tau[None]\n",
    "    threshold_val = threshold[None]\n",
    "    dt_val = dt[None]\n",
    "    \n",
    "    # Step 1: Compute synaptic currents (parallelized)\n",
    "    for post_neuron in range(n_neurons):\n",
    "        # Accumulate synaptic input\n",
    "        synaptic_current = 0.0\n",
    "        start = syn_indptr[post_neuron]\n",
    "        end = syn_indptr[post_neuron + 1]\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            pre_neuron = syn_indices[j]\n",
    "            if input_spikes[pre_neuron] > 0.5:  # Check if pre-synaptic neuron spiked\n",
    "                synaptic_current += syn_values[j]\n",
    "        \n",
    "        # Step 2: Update membrane potential\n",
    "        v_old = membrane_v[post_neuron]\n",
    "        v_new = lif_dynamics(v_old, synaptic_current, tau_val, dt_val)\n",
    "        \n",
    "        # Step 3: Check for spike and reset\n",
    "        spike = check_spike(v_new, threshold_val)\n",
    "        if spike:\n",
    "            v_new = 0.0  # Reset after spike\n",
    "        \n",
    "        # Write outputs\n",
    "        output_spikes[post_neuron] = ti.cast(spike, ti.f32)\n",
    "        new_membrane_v[post_neuron] = v_new\n",
    "\n",
    "# Register the operator\n",
    "snn_layer_op = bti.XLACustomOp(\n",
    "    cpu_kernel=snn_layer_optimized,\n",
    "    gpu_kernel=snn_layer_optimized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:31:55.657227Z",
     "start_time": "2025-10-29T09:31:54.511837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input spikes: 107.0/1000\n",
      "Output spikes: 0.0/800\n",
      "Mean membrane potential: 0.2725\n",
      "Max membrane potential: 0.5309\n"
     ]
    }
   ],
   "source": [
    "# Test the optimized SNN layer\n",
    "def test_snn_layer():\n",
    "    n_pre = 1000\n",
    "    n_post = 800\n",
    "    density = 0.1\n",
    "    \n",
    "    # Create random connectivity\n",
    "    conn = (np.random.rand(n_post, n_pre) < density).astype(float)\n",
    "    conn *= np.random.rand(n_post, n_pre) * 0.5  # Random weights\n",
    "    sparse_conn = csr_matrix(conn)\n",
    "    \n",
    "    # Initial states\n",
    "    input_spikes = (np.random.rand(n_pre) < 0.1).astype(np.float32)\n",
    "    membrane_v = np.random.rand(n_post).astype(np.float32) * 0.5\n",
    "    \n",
    "    # Parameters (as 0-dim arrays)\n",
    "    tau = jnp.array(10.0, dtype=jnp.float32)\n",
    "    threshold = jnp.array(1.0, dtype=jnp.float32)\n",
    "    dt = jnp.array(0.1, dtype=jnp.float32)\n",
    "    \n",
    "    # Run the SNN layer\n",
    "    output_spikes, new_v = snn_layer_op(\n",
    "        jnp.array(sparse_conn.data, dtype=jnp.float32),\n",
    "        jnp.array(sparse_conn.indices, dtype=jnp.int32),\n",
    "        jnp.array(sparse_conn.indptr, dtype=jnp.int32),\n",
    "        jnp.array(input_spikes, dtype=jnp.float32),\n",
    "        jnp.array(membrane_v, dtype=jnp.float32),\n",
    "        tau, threshold, dt,\n",
    "        outs=[\n",
    "            jax.ShapeDtypeStruct((n_post,), dtype=jnp.float32),\n",
    "            jax.ShapeDtypeStruct((n_post,), dtype=jnp.float32)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"Input spikes: {input_spikes.sum()}/{n_pre}\")\n",
    "    print(f\"Output spikes: {output_spikes.sum()}/{n_post}\")\n",
    "    print(f\"Mean membrane potential: {new_v.mean():.4f}\")\n",
    "    print(f\"Max membrane potential: {new_v.max():.4f}\")\n",
    "\n",
    "test_snn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial covered advanced optimization techniques for `braintaichi`:\n",
    "\n",
    "- Loop configuration and parallelization strategies\n",
    "- Memory access patterns and caching\n",
    "- Sparse operation optimization\n",
    "- Event-driven computation strategies\n",
    "- Data type selection\n",
    "- Atomic operations for thread safety\n",
    "- Code organization with helper functions\n",
    "- Complete optimized SNN layer example\n",
    "\n",
    "For more information:\n",
    "- [Taichi Documentation](https://docs.taichi-lang.org/)\n",
    "- [BrainTaichi API Reference](https://braintaichi.readthedocs.io/)\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
