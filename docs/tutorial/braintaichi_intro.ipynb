{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainTaichi Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial provides a comprehensive guide on how to develop custom operators using `BrainTaichi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Registration Interface\n",
    "\n",
    "Brain dynamics is sparse and event-driven, however, proprietary operators for brain dynamics are not well abstracted and summarized. As a result, we are often faced with the need to customize operators. In this tutorial, we will explore how to customize brain dynamics operators using `BrainTaichi`.\n",
    "\n",
    "Start by importing the relevant Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:30:50.640715Z",
     "start_time": "2025-10-30T02:30:48.637397Z"
    },
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:55.416042Z",
     "iopub.status.busy": "2025-10-30T03:02:55.416042Z",
     "iopub.status.idle": "2025-10-30T03:02:56.659283Z",
     "shell.execute_reply": "2025-10-30T03:02:56.659283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.3, llvm 15.0.1, commit 5ec301be, win, python 3.12.12\n",
      "[Taichi] Starting on arch=x64\n",
      "Taichi initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import taichi as ti\n",
    "import braintaichi as bti\n",
    "\n",
    "# Initialize Taichi\n",
    "ti.init(arch=ti.cpu)\n",
    "print(\"Taichi initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Structure of Custom Operators\n",
    "`Taichi` uses Python functions and decorators to define custom operators. Here is a basic structure of a custom operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:30:55.264153Z",
     "start_time": "2025-10-30T02:30:55.261152Z"
    },
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:56.660317Z",
     "iopub.status.busy": "2025-10-30T03:02:56.660317Z",
     "iopub.status.idle": "2025-10-30T03:02:56.663317Z",
     "shell.execute_reply": "2025-10-30T03:02:56.663317Z"
    }
   },
   "outputs": [],
   "source": [
    "@ti.kernel\n",
    "def my_kernel(arg1: ti.types.ndarray(ndim=1), arg2: ti.types.ndarray(ndim=1)):\n",
    "    # Internal logic of the operator\n",
    "    for i in range(arg1.shape[0]):\n",
    "        arg2[i] = arg1[i] * 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@ti.kernel` decorator tells Taichi that this is a function that requires special compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions\n",
    "When defining complex custom operators, you can use the @ti.func decorator to define helper functions. These functions can be called inside the kernel function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:30:59.252074Z",
     "start_time": "2025-10-30T02:30:59.248352Z"
    },
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:56.664504Z",
     "iopub.status.busy": "2025-10-30T03:02:56.664504Z",
     "iopub.status.idle": "2025-10-30T03:02:56.667685Z",
     "shell.execute_reply": "2025-10-30T03:02:56.667685Z"
    }
   },
   "outputs": [],
   "source": [
    "@ti.func\n",
    "def helper_func(x: ti.f32) -> ti.f32:\n",
    "    # Auxiliary computation\n",
    "    return x * 2\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def my_kernel_with_helper(arg: ti.types.ndarray(ndim=1)):\n",
    "    for i in range(arg.shape[0]):\n",
    "        arg[i] = helper_func(arg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Using Built-in BrainTaichi Operators\n",
    "\n",
    "Before creating custom operators, let's see how to use BrainTaichi's built-in operators. BrainTaichi provides optimized sparse matrix operations for brain dynamics simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:31:02.084222Z",
     "start_time": "2025-10-30T02:31:01.955588Z"
    },
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:56.668838Z",
     "iopub.status.busy": "2025-10-30T03:02:56.668838Z",
     "iopub.status.idle": "2025-10-30T03:02:56.756106Z",
     "shell.execute_reply": "2025-10-30T03:02:56.756106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse connectivity (CSR format):\n",
      "  indices: [1 3 0 2 4 1 2 4 0 3]\n",
      "  indptr: [ 0  2  5  6  8 10]\n",
      "  weight: 0.5\n",
      "\n",
      "Pre-synaptic spikes: [1. 0. 1. 0. 1.]\n",
      "  (Neurons 0, 2, 4 fired)\n",
      "\n",
      "Post-synaptic currents: 0.0\n",
      "Explanation: Each firing neuron sends weight to its targets\n"
     ]
    }
   ],
   "source": [
    "# Example: Using event-driven CSR matrix-vector multiplication\n",
    "# This is useful for spike propagation in neural networks\n",
    "\n",
    "# Create a small sparse connectivity matrix in CSR format\n",
    "# 5 neurons, sparse connections\n",
    "num_pre = 5\n",
    "num_post = 5\n",
    "\n",
    "# CSR format: indptr, indices, data\n",
    "# Row 0: connects to [1, 3]\n",
    "# Row 1: connects to [0, 2, 4]  \n",
    "# Row 2: connects to [1]\n",
    "# Row 3: connects to [2, 4]\n",
    "# Row 4: connects to [0, 3]\n",
    "\n",
    "indices = jnp.array([1, 3, 0, 2, 4, 1, 2, 4, 0, 3], dtype=jnp.int32)\n",
    "indptr = jnp.array([0, 2, 5, 6, 8, 10], dtype=jnp.int32)\n",
    "weight = jnp.array([0.5], dtype=jnp.float32)  # Homogeneous weight\n",
    "\n",
    "# Events: which pre-synaptic neurons fired\n",
    "events = jnp.array([1.0, 0.0, 1.0, 0.0, 1.0], dtype=jnp.float32)\n",
    "\n",
    "print(\"Sparse connectivity (CSR format):\")\n",
    "print(f\"  indices: {indices}\")\n",
    "print(f\"  indptr: {indptr}\")\n",
    "print(f\"  weight: {weight[0]}\")\n",
    "print(f\"\\nPre-synaptic spikes: {events}\")\n",
    "print(f\"  (Neurons 0, 2, 4 fired)\")\n",
    "\n",
    "# Use BrainTaichi's event CSR matrix-vector multiply\n",
    "result = bti.event_csrmv(\n",
    "    weight, \n",
    "    indices, \n",
    "    indptr, \n",
    "    events,\n",
    "    shape=(num_pre, num_post),\n",
    "    transpose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nPost-synaptic currents: {result[0]}\")\n",
    "print(\"Explanation: Each firing neuron sends weight to its targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Custom Event Processing Operator\n",
    "\n",
    "Now let's create a simple custom operator. We'll build an event-driven sparse operation using the ELL (ELLPACK) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:56.757109Z",
     "iopub.status.busy": "2025-10-30T03:02:56.757109Z",
     "iopub.status.idle": "2025-10-30T03:02:56.761388Z",
     "shell.execute_reply": "2025-10-30T03:02:56.761388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernels defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the Taichi kernels for CPU and GPU\n",
    "\n",
    "@ti.kernel\n",
    "def event_ell_cpu(\n",
    "    indices: ti.types.ndarray(ndim=2),\n",
    "    events: ti.types.ndarray(ndim=1),\n",
    "    weight: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    \"\"\"\n",
    "    Event-driven sparse matrix-vector multiply using ELL format.\n",
    "    Only processes rows where events[i] > 0.\n",
    "    \"\"\"\n",
    "    w = weight[0]\n",
    "    num_rows, num_cols = indices.shape\n",
    "    \n",
    "    # Serialize on CPU for correctness\n",
    "    ti.loop_config(serialize=True)\n",
    "    for i in range(num_rows):\n",
    "        if events[i] > 0.0:\n",
    "            # Process all connections for this row\n",
    "            for j in range(num_cols):\n",
    "                col_idx = indices[i, j]\n",
    "                if col_idx >= 0:  # Check for valid index\n",
    "                    out[col_idx] += w * events[i]\n",
    "\n",
    "\n",
    "@ti.kernel  \n",
    "def event_ell_gpu(\n",
    "    indices: ti.types.ndarray(ndim=2),\n",
    "    events: ti.types.ndarray(ndim=1),\n",
    "    weight: ti.types.ndarray(ndim=1),\n",
    "    out: ti.types.ndarray(ndim=1)\n",
    "):\n",
    "    \"\"\"\n",
    "    GPU version: parallel across rows\n",
    "    \"\"\"\n",
    "    w = weight[0]\n",
    "    num_rows, num_cols = indices.shape\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        if events[i] > 0.0:\n",
    "            for j in range(num_cols):\n",
    "                col_idx = indices[i, j]\n",
    "                if col_idx >= 0:\n",
    "                    out[col_idx] += w * events[i]\n",
    "\n",
    "print(\"Kernels defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:31:09.191003Z",
     "start_time": "2025-10-30T02:31:09.108011Z"
    },
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:56.762392Z",
     "iopub.status.busy": "2025-10-30T03:02:56.762392Z",
     "iopub.status.idle": "2025-10-30T03:02:56.765485Z",
     "shell.execute_reply": "2025-10-30T03:02:56.765485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator registered successfully\n",
      "\n",
      "ELL format connectivity:\n",
      "  indices shape: 5x3\n",
      "  events: [1.0, 0.0, 1.0, 0.0, 1.0]\n",
      "  weight: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Register the operator with BrainTaichi\n",
    "# This makes it compatible with JAX's JIT, grad, vmap, etc.\n",
    "\n",
    "event_ell_op = bti.XLACustomOp(\n",
    "    cpu_kernel=event_ell_cpu,\n",
    "    gpu_kernel=event_ell_gpu,\n",
    "    name='event_ell_custom'\n",
    ")\n",
    "\n",
    "print(\"Operator registered successfully\")\n",
    "\n",
    "# Step 3: Create test data in ELL format\n",
    "num_neurons = 5\n",
    "max_connections = 3\n",
    "\n",
    "# ELL format: each row has fixed width (max_connections)\n",
    "# -1 indicates no connection\n",
    "ell_indices = [\n",
    "    [1, 3, -1],    # Neuron 0 connects to [1, 3]\n",
    "    [0, 2, 4],     # Neuron 1 connects to [0, 2, 4]\n",
    "    [1, -1, -1],   # Neuron 2 connects to [1]\n",
    "    [2, 4, -1],    # Neuron 3 connects to [2, 4]\n",
    "    [0, 3, -1],    # Neuron 4 connects to [0, 3]\n",
    "]\n",
    "\n",
    "# Same events as before\n",
    "events = [1.0, 0.0, 1.0, 0.0, 1.0]  # Neurons 0, 2, 4 fire\n",
    "weight = [0.5]\n",
    "\n",
    "print(f\"\\nELL format connectivity:\")\n",
    "print(f\"  indices shape: {len(ell_indices)}x{len(ell_indices[0])}\")\n",
    "print(f\"  events: {events}\")\n",
    "print(f\"  weight: {weight[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the custom operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:56.766488Z",
     "iopub.status.busy": "2025-10-30T03:02:56.766488Z",
     "iopub.status.idle": "2025-10-30T03:02:56.951174Z",
     "shell.execute_reply": "2025-10-30T03:02:56.951174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT-compiled result: [0.5 1.  0.  1.  0. ]\n",
      "\n",
      "The JIT-compiled version will be faster on subsequent calls!\n"
     ]
    }
   ],
   "source": [
    "## JAX Integration Features\n",
    "\n",
    "# One of the key benefits of using BrainTaichi's XLACustomOp is seamless \n",
    "# integration with JAX. Your custom operators automatically support:\n",
    "# - JIT compilation: Speed up execution\n",
    "# - Automatic differentiation: For gradient-based learning\n",
    "# - Vectorization (vmap): Process batches efficiently\n",
    "\n",
    "# Example: JIT compilation for faster execution\n",
    "@jax.jit\n",
    "def run_network_jit(indices, events, weight):\n",
    "    \"\"\"JIT-compiled function using our custom operator\"\"\"\n",
    "    return event_ell_op(\n",
    "        indices,\n",
    "        events, \n",
    "        weight,\n",
    "        outs=[jax.ShapeDtypeStruct((num_neurons,), dtype=jnp.float32)]\n",
    "    )[0]\n",
    "\n",
    "# First call compiles the function\n",
    "result_jit = run_network_jit(\n",
    "    jnp.array(ell_indices, dtype=jnp.int32),\n",
    "    jnp.array(events, dtype=jnp.float32),\n",
    "    jnp.array(weight, dtype=jnp.float32)\n",
    ")\n",
    "\n",
    "print(\"JIT-compiled result:\", result_jit)\n",
    "print(\"\\nThe JIT-compiled version will be faster on subsequent calls!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Taichi Concepts\n",
    "\n",
    "Taichi is a domain-specific language (DSL) embedded in Python, designed for high-performance computing. Here are key concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Embedded in Python\n",
    "\n",
    "Taichi is embedded within Python, letting you leverage Python's simplicity while getting native GPU/CPU performance. If you know Python, you can start using Taichi immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Just-in-Time (JIT) Compilation\n",
    "\n",
    "Taichi uses JIT compilation (LLVM, SPIR-V) to translate Python code into native GPU or CPU instructions, ensuring high performance at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Imperative Programming\n",
    "\n",
    "Unlike many DSLs, Taichi uses an imperative programming paradigm, giving you flexibility to write complex computations in a single \"mega-kernel\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compiler Optimizations\n",
    "\n",
    "Taichi employs various optimizations (common subexpression elimination, dead code elimination, control flow analysis) that are backend-neutral thanks to its own IR layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Optimization in Taichi\n",
    "\n",
    "Taichi kernels automatically parallelize outermost for-loops. However, you can fine-tune performance using `ti.loop_config`:\n",
    "\n",
    "- **parallelize**: Set number of CPU threads\n",
    "- **block_dim**: Set GPU block size  \n",
    "- **serialize**: Run loop serially (allows break statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Optimization in Taichi\n",
    "\n",
    "`Taichi` kernels automatically parallelize for-loops in the outermost scope. Our compiler sets the settings automatically to best explore the target architecture. Nonetheless, for Ninjas seeking the final few percent of speed, we provide several APIs to allow developers to fine-tune their programs. Specifying a proper block_dim is key.\n",
    "\n",
    "You can use `ti.loop_config` to set the loop directives for the next for loop. Available directives are:\n",
    "\n",
    "- **parallelize**: Sets the number of threads to use on CPU\n",
    "- **block_dim**: Sets the number of threads in a block on GPU\n",
    "- **serialize**: If you set **serialize** to True, the for loop will run serially, and you can write break statements inside it (Only applies on range/ndrange fors). Equals to setting **parallelize** to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:56.953177Z",
     "iopub.status.busy": "2025-10-30T03:02:56.953177Z",
     "iopub.status.idle": "2025-10-30T03:02:56.971359Z",
     "shell.execute_reply": "2025-10-30T03:02:56.971359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of break_in_serial_for: 55\n",
      "First 10 values: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "@ti.kernel\n",
    "def break_in_serial_for() -> ti.i32:\n",
    "    a = 0\n",
    "    ti.loop_config(serialize=True)\n",
    "    for i in range(100):  # This loop runs serially\n",
    "        a += i\n",
    "        if i == 10:\n",
    "            break\n",
    "    return a\n",
    "\n",
    "\n",
    "result = break_in_serial_for()  # returns 55\n",
    "print(f\"Result of break_in_serial_for: {result}\")\n",
    "\n",
    "\n",
    "# Example of using loop_config with ndarray\n",
    "@ti.kernel\n",
    "def fill_array(val: ti.types.ndarray(ndim=1)):\n",
    "    n = val.shape[0]\n",
    "    ti.loop_config(parallelize=8)\n",
    "    # If the kernel is run on the CPU backend, 8 threads will be used to run it\n",
    "    for i in range(n):\n",
    "        val[i] = i\n",
    "\n",
    "\n",
    "n = 128\n",
    "val_array = np.zeros(n, dtype=np.int32)\n",
    "fill_array(val_array)\n",
    "print(f\"First 10 values: {val_array[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braintaichi_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
